{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383925f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##jsonl - csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7fbe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymysql sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed50ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob \n",
    "\n",
    "\n",
    "input_folder_path = r'C:\\Users\\Rohan\\Downloads\\fermbile\\Datasets-2025-08-08'\n",
    "\n",
    "output_csv_file = 'full_news_events_data_combined.csv'\n",
    "\n",
    "all_flattened_data = []\n",
    "\n",
    "\n",
    "def safe_extract(record, keys):\n",
    "    current = record\n",
    "    for key in keys:\n",
    "        if isinstance(current, dict) and key in current:\n",
    "            current = current[key]\n",
    "        else:\n",
    "            return None\n",
    "   \n",
    "    if isinstance(current, list):\n",
    "        if all(isinstance(item, (str, int, float)) for item in current):\n",
    "            return \" | \".join(map(str, current))\n",
    "        \n",
    "        elif keys[-1] == 'location_data':\n",
    "            locations = []\n",
    "            for loc in current:\n",
    "                parts = [loc.get('city'), loc.get('country')]\n",
    "                locations.append(\", \".join(filter(None, parts)))\n",
    "            return \" | \".join(locations)\n",
    "       \n",
    "        else:\n",
    "            try:\n",
    "               \n",
    "                return json.dumps(current, ensure_ascii=False) \n",
    "            except:\n",
    "                return str(current)\n",
    "    return current\n",
    "\n",
    "\n",
    "jsonl_files = glob.glob(os.path.join(input_folder_path, '*.jsonl'))\n",
    "\n",
    "if not jsonl_files:\n",
    "    print(f\" No .jsonl files found in the directory: {input_folder_path}\")\n",
    "else:\n",
    "    print(f\"Processing {len(jsonl_files)} .jsonl files...\")\n",
    "\n",
    "    \n",
    "    for file_name in jsonl_files:\n",
    "        print(f\"   Reading {file_name}...\")\n",
    "        data = []\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            \n",
    "            with open(file_name, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    \n",
    "                    if line.strip():\n",
    "                        data.append(json.loads(line))\n",
    "        except Exception as e:\n",
    "            print(f\"    Error reading JSONL file {file_name}: {e}. Skipping file.\")\n",
    "            continue\n",
    "\n",
    "        #  Extract and flatten the data from the current file\n",
    "        for record in data:\n",
    "            try:\n",
    "                events = record.get('data', [])\n",
    "                \n",
    "                for event in events:\n",
    "                    if event.get('type') == 'news_event':\n",
    "                        attributes = event.get('attributes', {})\n",
    "                        relationships = event.get('relationships', {})\n",
    "                        \n",
    "                        # Extract event attributes\n",
    "                        row = {\n",
    "                            'event_id': event.get('id'),\n",
    "                            'event_category': attributes.get('category'),\n",
    "                            'event_summary': attributes.get('summary'),\n",
    "                            'effective_date': attributes.get('effective_date'),\n",
    "                            'found_at': attributes.get('found_at'),\n",
    "                            'location': attributes.get('location'),\n",
    "                            'award': attributes.get('award'),\n",
    "                            'recognition': attributes.get('recognition'),\n",
    "                            'product': attributes.get('product'),\n",
    "                            'article_sentence': attributes.get('article_sentence'),\n",
    "                            'source_file': os.path.basename(file_name) # Add source file for traceability\n",
    "                        }\n",
    "\n",
    "                       \n",
    "                        company_id = safe_extract(relationships, ['company1', 'data', 'id'])\n",
    "                        if company_id:\n",
    "                            \n",
    "                            company_info = next((item['attributes'] for item in record.get('included', []) \n",
    "                                                 if item.get('id') == company_id and item.get('type') == 'company'), {})\n",
    "                            row['company_name'] = company_info.get('company_name')\n",
    "                            row['company_domain'] = company_info.get('domain')\n",
    "                            row['company_ticker'] = company_info.get('ticker')\n",
    "                        else:\n",
    "                            row['company_name'] = None\n",
    "                            row['company_domain'] = None\n",
    "                            row['company_ticker'] = None\n",
    "\n",
    "                       \n",
    "                        article_id = safe_extract(relationships, ['most_relevant_source', 'data', 'id'])\n",
    "                        if article_id:\n",
    "                           \n",
    "                            article_info = next((item['attributes'] for item in record.get('included', []) \n",
    "                                                 if item.get('id') == article_id and item.get('type') == 'news_article'), {})\n",
    "                            row['article_title'] = article_info.get('title')\n",
    "                            row['article_url'] = article_info.get('url')\n",
    "                        else:\n",
    "                            row['article_title'] = None\n",
    "                            row['article_url'] = None\n",
    "                        \n",
    "                        all_flattened_data.append(row)\n",
    "\n",
    "            except Exception as e:\n",
    "                \n",
    "                continue\n",
    "\n",
    "   \n",
    "    if all_flattened_data:\n",
    "        df = pd.DataFrame(all_flattened_data)\n",
    "        \n",
    "        \n",
    "        final_columns = [\n",
    "            'event_id', 'source_file', 'company_name', 'company_domain', 'company_ticker',\n",
    "            'event_category', 'event_summary', 'effective_date', 'found_at',\n",
    "            'location', 'award', 'recognition', 'product', 'article_title', 'article_url',\n",
    "            'article_sentence'\n",
    "        ]\n",
    "        \n",
    "       \n",
    "        df = df.reindex(columns=final_columns)\n",
    "\n",
    "        \n",
    "        df.to_csv(output_csv_file, index=False, encoding='utf-8')\n",
    "        print(f\"\\n Successfully processed {len(jsonl_files)} files and saved {len(df)} events to **{output_csv_file}**.\")\n",
    "    else:\n",
    "        print(\"\\n No news events were successfully extracted from any of the files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28207910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from urllib.parse import urlparse\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d93de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##loading data\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\Rohan\\Downloads\\fermbile\\full_news_events_data_combined.csv\")\n",
    "print(\"\\n Data Loaded Successfully!\")\n",
    "print(f\"Total Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d0c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "##summary\n",
    "\n",
    "def profile_dataframe(df):\n",
    "    summary = pd.DataFrame({\n",
    "        'dtype': df.dtypes,\n",
    "        'missing_count': df.isna().sum(),\n",
    "        'missing_%': (df.isna().mean() * 100).round(2),\n",
    "        'unique_count': df.nunique()\n",
    "    })\n",
    "    return summary\n",
    "\n",
    "profile_summary = profile_dataframe(df)\n",
    "print(\"\\n Data Profiling Summary:\")\n",
    "profile_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b33c044",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data quality check\n",
    "\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def check_data_quality(df):\n",
    "    dq_report = {}\n",
    "\n",
    "    dq_report['missing_values_%'] = df.isna().mean().mean() * 100\n",
    "    dq_report['duplicate_rows'] = df.duplicated().sum()\n",
    "\n",
    "    if 'article_url' in df.columns:\n",
    "        invalid_urls = df['article_url'].apply(lambda x: not is_valid_url(str(x)) if pd.notnull(x) else False).sum()\n",
    "        dq_report['invalid_urls'] = invalid_urls\n",
    "\n",
    "    return dq_report\n",
    "\n",
    "dq_before = check_data_quality(df)\n",
    "print(\"\\n Data Quality Issues (Before Cleaning):\")\n",
    "print(dq_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee15b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data cleaning\n",
    "\n",
    "clean_df = df.copy()\n",
    "\n",
    "\n",
    "clean_df.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "for col in clean_df.select_dtypes(include='object').columns:\n",
    "    clean_df[col] = clean_df[col].astype(str).str.strip()\n",
    "\n",
    "\n",
    "essential_cols = [col for col in ['title', 'article_url'] if col in clean_df.columns]\n",
    "clean_df.dropna(subset=essential_cols, inplace=True)\n",
    "\n",
    "\n",
    "if 'article_url' in clean_df.columns:\n",
    "    clean_df['article_url'] = clean_df['article_url'].apply(lambda x: str(x).strip().lower().rstrip('/'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data quality summary\n",
    "\n",
    "dq_after = check_data_quality(clean_df)\n",
    "\n",
    "print(\"\\n Data Quality Comparison:\")\n",
    "comparison = pd.DataFrame([dq_before, dq_after], index=['Before', 'After']).T\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9798aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Visualization of Data Quality\n",
    "\n",
    "missing = df.isna().mean() * 100\n",
    "plt.figure(figsize=(10,4))\n",
    "missing.sort_values(ascending=False).plot(kind='bar', color='tomato')\n",
    "plt.title('Percentage of Missing Values by Column')\n",
    "plt.ylabel('% Missing')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=comparison.columns, y=comparison.loc['missing_values_%'])\n",
    "plt.title('Missing Data Before vs After Cleaning')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00a0bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv('Cleaned_data.csv', index=False)\n",
    "print(\"\\n Cleaned data saved as 'Cleaned_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d86db41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "mysql_config = {\n",
    "    'host': 'localhost',\n",
    "    'user': 'root',\n",
    "    'password': '########',\n",
    "    'database': 'news_events_db'\n",
    "}\n",
    "\n",
    "\n",
    "conn_str = (\n",
    "    f\"mysql+pymysql://{mysql_config['user']}:{mysql_config['password']}@\"\n",
    "    f\"{mysql_config['host']}/{mysql_config['database']}\"\n",
    ")\n",
    "\n",
    "\n",
    "engine = create_engine(conn_str)\n",
    "print(\" MySQL connection engine created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0201ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload cleaned data to MySQL\n",
    "try:\n",
    "    clean_df.to_sql(\n",
    "        name='news_events_cleaned',   # table name\n",
    "        con=engine,                   # connection engine created earlier\n",
    "        if_exists='replace',          # overwrite existing table (use 'append' if you want to add)\n",
    "        index=False                   # don't write pandas index\n",
    "    )\n",
    "    print(\" Cleaned data uploaded to MySQL successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\" Upload failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3180adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dq_summary = {\n",
    "    'run_id': '001',\n",
    "    'rows_before': len(df),\n",
    "    'rows_after': len(clean_df),\n",
    "    'duplicates_removed': len(df) - len(clean_df),\n",
    "}\n",
    "\n",
    "dq_df = pd.DataFrame([dq_summary])\n",
    "dq_df.to_sql('dq_metrics_run', con=engine, if_exists='append', index=False)\n",
    "print(\" Metrics inserted into dq_metrics_run!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6023fe41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
